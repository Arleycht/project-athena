{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up proper directory paths\n",
    "\n",
    "project_path = Path().absolute().parent\n",
    "src_path = project_path.joinpath(\"src\")\n",
    "\n",
    "# Ensure the paths are properly assigned\n",
    "# If this assertion fails, change project_dir as needed to become the project directory\n",
    "# If project_dir is correct, change the name in the assertion check\n",
    "assert project_path.name == \"project-athena\", \"Parent directory name assertion failed (check the path)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src_dir to module paths\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks.attack import generate\n",
    "from models.athena import ENSEMBLE_STRATEGY, Ensemble\n",
    "from utils.data import subsampling\n",
    "from utils.file import load_from_json, dump_to_json\n",
    "from utils.metrics import error_rate, get_corrections\n",
    "from utils.model import load_lenet, load_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from tutorials/craft_adversarial_examples.py\n",
    "def generate_ae(model, data, labels, attack_configs,\n",
    "                eot=False,\n",
    "                save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples\n",
    "    :param model: WeakDefense. The targeted model.\n",
    "    :param data: array. The benign samples to generate adversarial for.\n",
    "    :param labels: array or list. The true labels.\n",
    "    :param attack_configs: dictionary. Attacks and corresponding settings.\n",
    "    :param save: boolean. True, if save the adversarial examples.\n",
    "    :param output_dir: str or path. Location to save the adversarial examples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img_rows, img_cols = data.shape[1], data.shape[2]\n",
    "    num_attacks = attack_configs.get(\"num_attacks\")\n",
    "    data_loader = (data, labels)\n",
    "\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = np.asarray([np.argmax(p) for p in labels])\n",
    "\n",
    "    # generate attacks one by one\n",
    "    for id in range(num_attacks):\n",
    "        key = \"configs{}\".format(id)\n",
    "        attack_args = attack_configs.get(key)\n",
    "        \n",
    "        attack_args[\"eot\"] = eot\n",
    "        data_adv = generate(model=model,\n",
    "                            data_loader=data_loader,\n",
    "                            attack_args=attack_args\n",
    "                            )\n",
    "        # predict the adversarial examples\n",
    "        predictions = model.predict(data_adv)\n",
    "        predictions = np.asarray([np.argmax(p) for p in predictions])\n",
    "\n",
    "        err = error_rate(y_pred=predictions, y_true=labels)\n",
    "        print(\">>> error rate:\", err)\n",
    "\n",
    "        # plotting some examples\n",
    "        num_plotting = min(data.shape[0], 2)\n",
    "        for i in range(num_plotting):\n",
    "            img = data_adv[i].reshape((img_rows, img_cols))\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            title = '{}(EOT:{}): {}->{}'.format(attack_configs.get(key).get(\"description\"),\n",
    "                                                \"ON\" if eot else \"OFF\",\n",
    "                                                labels[i],\n",
    "                                                predictions[i]\n",
    "                                                )\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # save the adversarial example\n",
    "        if save:\n",
    "            if output_dir is None:\n",
    "                raise ValueError(\"Cannot save images to a none path.\")\n",
    "            # save with a random name\n",
    "            file = os.path.join(output_dir, \"{}.npy\".format(time.monotonic()))\n",
    "            print(\"Save the adversarial examples to file [{}].\".format(file))\n",
    "            np.save(file, data_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading model [../models/cnn/model-mnist-cnn-morph_gradient.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-distort_x.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-noise_gaussian.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-noise_sNp.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-filter_gaussian.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-geo_iradon.h5]...\n",
      ">>> Loading model [../models/cnn/model-mnist-cnn-seg_gradient.h5]...\n",
      ">>> Loaded 7 models.\n"
     ]
    }
   ],
   "source": [
    "# Load data configs\n",
    "file = project_path.joinpath(\"Task2/configs/data-mnist.json\")\n",
    "data_configs = load_from_json(file)\n",
    "\n",
    "trans_configs = load_from_json(project_path.joinpath(\"Task2/configs/athena-mnist.json\"))\n",
    "model_configs = load_from_json(project_path.joinpath(\"Task2/configs/model-mnist.json\"))\n",
    "\n",
    "# In the context of the white-box threat model,\n",
    "# we use the ensemble as adversary's target model.\n",
    "pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                    model_configs=model_configs,\n",
    "                    active_list=True,\n",
    "                    wrap=True)\n",
    "\n",
    "# create an AVEP ensemble from the WD pool\n",
    "wds = list(pool.values())\n",
    "target = Ensemble(classifiers=wds, strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "\n",
    "# load the benign samples\n",
    "data_file = project_path.joinpath(data_configs['dir'], data_configs.get('bs_file'))\n",
    "data_bs = np.load(data_file)\n",
    "# load the corresponding true labels\n",
    "label_file = project_path.joinpath(data_configs['dir'], data_configs.get('label_file'))\n",
    "labels = np.load(label_file)\n",
    "\n",
    "\n",
    "output_path = project_path.joinpath(\"Task2/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_configs = load_from_json(project_path.joinpath(\"Task2/configs/attack-zk-mnist.json\"))\n",
    "generate_ae(model=target, data=data_bs,labels=labels, attack_configs=attack_configs,\n",
    "            save=True, output_dir=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "attack_configs = load_from_json(project_path.joinpath(\"Task2/configs/attack-zk-mnistEOTON.json\"))\n",
    "\n",
    "# Adaptive approach (with EOT)\n",
    "# Compute the loss expectation over specific distribution.\n",
    "# For an ensemble target, averaging the EOT of WDs'.\n",
    "# let save=True and specify an output folder to save the generated AEs\n",
    "generate_ae(model=target, data=data_bs, labels=labels, eot=True, attack_configs=attack_configs, save=True, output_dir=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the generated adverserial example on the vanilla Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covin\\anaconda3\\envs\\athena_proj2\\lib\\site-packages\\skimage\\transform\\radon_transform.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  coords = np.array(np.ogrid[:image.shape[0], :image.shape[1]])\n",
      "C:\\Users\\covin\\anaconda3\\envs\\athena_proj2\\lib\\site-packages\\skimage\\transform\\radon_transform.py:91: UserWarning: Radon transform: image must be zero outside the reconstruction circle\n",
      "  warn('Radon transform: image must be zero outside the '\n",
      "C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\src\\models\\image_processor.py:856: FutureWarning: 'filter' is a deprecated argument name for `iradon`. It will be removed in version 0.19. Please use 'filter_name' instead.\n",
      "  interpolation=interpolation, circle=circle))\n",
      "C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\src\\models\\image_processor.py:68: UserWarning: Possible precision loss converting image of type float32 to uint8 as required by rank filters. Convert manually using skimage.util.img_as_ubyte to silence this warning.\n",
      "  return _segment_trans(X, trans_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-FGSM_0.5.npy]\n",
      ">>> Error Rate of task2-FGSM_0.5.npy: [0.8727272727272727]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-FGSM_eps0.1.npy]\n",
      ">>> Error Rate of task2-FGSM_eps0.1.npy: [0.012121212121212121]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-FGSM_eps0.01.npy]\n",
      ">>> Error Rate of task2-FGSM_eps0.01.npy: [0.00404040404040404]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-FGSM_eps0.2.npy]\n",
      ">>> Error Rate of task2-FGSM_eps0.2.npy: [0.05858585858585859]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-FGSM_eps0.05.npy]\n",
      ">>> Error Rate of task2-FGSM_eps0.05.npy: [0.00808080808080808]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-PGD_eps0.1.npy]\n",
      ">>> Error Rate of task2-PGD_eps0.1.npy: [0.024242424242424242]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-PGD_eps0.2.npy]\n",
      ">>> Error Rate of task2-PGD_eps0.2.npy: [0.12323232323232323]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-PGD_eps0.5.npy]\n",
      ">>> Error Rate of task2-PGD_eps0.5.npy: [0.8888888888888888]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-PGD_eps0.05.npy]\n",
      ">>> Error Rate of task2-PGD_eps0.05.npy: [0.00808080808080808]\n",
      ">>> Running evaluations on [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\data\\task2-PGD_eps0.7.npy]\n",
      ">>> Error Rate of task2-PGD_eps0.7.npy: [0.9696969696969697]\n",
      ">>> Evaluations on all ae_files dumped to [C:\\Users\\covin\\PycharmProjects\\585 Project\\project-athena\\Task2\\results\\ae_evaluation_results.json]\n"
     ]
    }
   ],
   "source": [
    "file = project_path.joinpath(\"Task2/configs/data-mnist.json\")\n",
    "data_configs = load_from_json(file)\n",
    "\n",
    "img_rows, img_cols = data_bs.shape[1], data_bs.shape[2]\n",
    "\n",
    "pred_bs = target.predict(data_bs)\n",
    "corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "results = {}\n",
    "\n",
    "ae_files = data_configs.get('task2_aes')\n",
    "\n",
    "for file in ae_files:\n",
    "    results[file] = {}\n",
    "    ae_file = project_path.joinpath(data_configs['dir'], file)\n",
    "    x_adversarial = np.load(ae_file)\n",
    "    print(f\">>> Running evaluations on [{ae_file}]\")\n",
    "    pred = target.predict(x_adversarial)\n",
    "    err = error_rate(y_pred=pred, y_true=labels, correct_on_bs=corrections)\n",
    "    print(f\">>> Error Rate of {file}: [{err}]\")\n",
    "    results[file] = err\n",
    "    \n",
    "result_file = project_path.joinpath(\"Task2/results/ae_evaluation_results.json\")\n",
    "dump_to_json(results, result_file)\n",
    "print(f\">>> Evaluations on all ae_files dumped to [{result_file}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
