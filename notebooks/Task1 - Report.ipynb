{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\"Generate adversarial examples in the context of the zero-knowledge threat model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up proper directory paths\n",
    "\n",
    "project_path = Path().absolute().parent\n",
    "src_path = project_path.joinpath(\"src\")\n",
    "\n",
    "# Ensure the paths are properly assigned\n",
    "# If this assertion fails, change project_dir as needed to become the project directory\n",
    "# If project_dir is correct, change the name in the assertion check\n",
    "assert project_path.name == \"project-athena\", \"Parent directory name assertion failed (check the path)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src_dir to module paths\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'art'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-44ff4bf23205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mattacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mathena\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mENSEMBLE_STRATEGY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEnsemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdump_to_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_corrections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\585 Project\\project-athena\\src\\attacks\\attack.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevasion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_gradient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFastGradientMethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevasion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcarlini\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCarliniL2Method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCarliniLInfMethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevasion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojected_gradient_descent\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProjectedGradientDescent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'art'"
     ]
    }
   ],
   "source": [
    "from attacks.attack import generate\n",
    "from models.athena import ENSEMBLE_STRATEGY, Ensemble\n",
    "from utils.data import subsampling\n",
    "from utils.file import load_from_json, dump_to_json\n",
    "from utils.metrics import error_rate, get_corrections\n",
    "from utils.model import load_lenet, load_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ae(model, data, labels, attack_configs, save=False, output_dir=None, device=None):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples for a model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generating adversarial examples\")\n",
    "    \n",
    "    ae_files = []\n",
    "    \n",
    "    num_attacks = attack_configs.get(\"num_attacks\")\n",
    "    data_loader = (data, labels)\n",
    "    \n",
    "    # For plotting\n",
    "    img_rows, img_cols = data.shape[1], data.shape[2]\n",
    "\n",
    "    if len(labels.shape) > 1:\n",
    "        labels = np.asarray([np.argmax(p) for p in labels])\n",
    "\n",
    "    # Generate attacks\n",
    "    for attack_id in range(num_attacks):\n",
    "        key = f\"configs{attack_id}\"\n",
    "        \n",
    "        data_adversarial = generate(model=model,\n",
    "                                    data_loader=data_loader,\n",
    "                                    attack_args=attack_configs[key],\n",
    "                                    device=device)\n",
    "        \n",
    "        # Evaluate adversarial examples\n",
    "        y_pred = model.predict(data_adversarial)\n",
    "        y_pred = np.asarray([np.argmax(p) for p in y_pred])\n",
    "\n",
    "        err = error_rate(y_pred=y_pred, y_true=labels)\n",
    "        \n",
    "        print(f\">>> error rate: {err}\")\n",
    "\n",
    "        # Save adversarial examples\n",
    "        if save:\n",
    "            if output_dir is None:\n",
    "                raise ValueError(\"Cannot save images to a none path.\")\n",
    "            \n",
    "            file_name = f\"task1-{attack_configs[key]['description']}-{time.monotonic()}.npy\"\n",
    "            file_path = Path(output_dir).joinpath(file_name)\n",
    "            np.save(file_path, data_adversarial)\n",
    "            \n",
    "            ae_files.append(file_name)\n",
    "            \n",
    "            print(f\"Saved adversarial example to file [{file_path}]\")\n",
    "        \n",
    "        print((attack_configs[key]['attack']))\n",
    "        \n",
    "        batch_size = min(data.shape[0], 2)\n",
    "        fig = plt.figure(figsize=(1, batch_size), dpi=300)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            fig.add_subplot(1, batch_size, i + 1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(data_adversarial[i].reshape((img_rows, img_cols)), cmap='gray')\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "        plt.savefig('results.pdf')\n",
    "    \n",
    "    print(\"Done generating adversarial examples\")\n",
    "    \n",
    "    return ae_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trans_configs, model_configs, data_configs,\n",
    "             save=False, output_dir=None):\n",
    "    \"\"\"\n",
    "    Apply transformation(s) on images.\n",
    "    :param trans_configs: dictionary. The collection of the parameterized transformations to test.\n",
    "        in the form of\n",
    "        { configsx: {\n",
    "            param: value,\n",
    "            }\n",
    "        }\n",
    "        The key of a configuration is 'configs'x, where 'x' is the id of corresponding weak defense.\n",
    "    :param model_configs:  dictionary. Defines model related information.\n",
    "        Such as, location, the undefended model, the file format, etc.\n",
    "    :param data_configs: dictionary. Defines data related information.\n",
    "        Such as, location, the file for the true labels, the file for the benign samples,\n",
    "        the files for the adversarial examples, etc.\n",
    "    :param save: boolean. Save the transformed sample or not.\n",
    "    :param output_dir: path or str. The location to store the transformed samples.\n",
    "        It cannot be None when save is True.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load baseline defense (PGD-ADT model)\n",
    "    baseline = load_lenet(file=model_configs['pgd_trained'],\n",
    "                          trans_configs=None,\n",
    "                          use_logits=False,\n",
    "                          wrap=False)\n",
    "\n",
    "    # Load undefended model (UM)\n",
    "    file = project_path.joinpath(model_configs['dir'], model_configs['um_file'])\n",
    "    undefended = load_lenet(file=file,\n",
    "                            trans_configs=trans_configs.get('configs0'),\n",
    "                            wrap=True)\n",
    "    \n",
    "    print(f\">>> UM: {type(undefended)}\")\n",
    "\n",
    "    # Load weak defenses into a pool\n",
    "    pool, _ = load_pool(trans_configs=trans_configs,\n",
    "                        model_configs=model_configs,\n",
    "                        active_list=True,\n",
    "                        wrap=True)\n",
    "    \n",
    "    # Create an AVEP ensemble from the WD pool\n",
    "    wds = list(pool.values())\n",
    "    ensemble = Ensemble(classifiers=wds, strategy=ENSEMBLE_STRATEGY.AVEP.value)\n",
    "    \n",
    "    print(f\">>> wds: {type(wds)} {type(wds[0])}\")\n",
    "\n",
    "    # Load benign samples\n",
    "    bs_file = project_path.joinpath(data_configs['dir'], data_configs['bs_file'])\n",
    "    \n",
    "    # Hacky workaround for benign data being in different directory as adversarial data\n",
    "    if 'benign_dir' in data_configs:\n",
    "        bs_file = project_path.joinpath(data_configs['benign_dir'], data_configs['bs_file'])\n",
    "    \n",
    "    x_bs = np.load(bs_file)\n",
    "    img_rows, img_cols = x_bs.shape[1], x_bs.shape[2]\n",
    "\n",
    "    # Load true labels\n",
    "    label_file = project_path.joinpath(data_configs['dir'], data_configs['label_file'])\n",
    "    labels = np.load(label_file)\n",
    "\n",
    "    print(f\">>> Evaluating UM on [{bs_file}]\")\n",
    "    \n",
    "    # Get indices of benign samples that are correctly classified by the targeted model\n",
    "    pred_bs = undefended.predict(x_bs)\n",
    "    corrections = get_corrections(y_pred=pred_bs, y_true=labels)\n",
    "\n",
    "    # Evaluate adversarial examples\n",
    "    results = {}\n",
    "    \n",
    "    # Only run on one example\n",
    "    ae_files = data_configs.get('ae_files')\n",
    "    ae_file = project_path.joinpath(data_configs['dir'], ae_files[4])\n",
    "    \n",
    "    for file in ae_files:\n",
    "        results[file] = {}\n",
    "        \n",
    "        ae_file = project_path.joinpath(data_configs['dir'], file)\n",
    "        x_adversarial = np.load(ae_file)\n",
    "        \n",
    "        print(f\">>> Running evaluations on [{ae_file}]\")\n",
    "\n",
    "        print(f\">>> Evaluating UM\")\n",
    "\n",
    "        # Evaluate undefended model on adversarial examples\n",
    "        pred = undefended.predict(x_adversarial)\n",
    "        err = error_rate(y_pred=pred, y_true=labels, correct_on_bs=corrections)\n",
    "        results[file]['UM'] = err\n",
    "\n",
    "        print(f\">>> Evaluating ensemble\")\n",
    "\n",
    "        # Evaluate ensemble model on adversarial examples\n",
    "        pred = ensemble.predict(x_adversarial)\n",
    "        err = error_rate(y_pred=pred, y_true=labels, correct_on_bs=corrections)\n",
    "        results[file]['Ensemble'] = err\n",
    "        \n",
    "        print(f\">>> Evaluating baseline model\")\n",
    "\n",
    "        # Evaluate baseline model on adversarial examples\n",
    "        pred = baseline.predict(x_adversarial)\n",
    "        err = error_rate(y_pred=pred, y_true=labels, correct_on_bs=corrections)\n",
    "        results[file]['PGD-ADT'] = err\n",
    "\n",
    "        print(f\">>> Evaluations on [{ae_file}]:\\n{results[file]}\")\n",
    "    \n",
    "    result_file = project_path.joinpath(\"results/ae_evaluation_results.json\")\n",
    "    dump_to_json(results, result_file)\n",
    "    \n",
    "    print(f\">>> Evaluations on all ae_files dumped to [{result_file}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a7222af2a8f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load data configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"configs/demo/data-mnist.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_configs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load model configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data configs\n",
    "file = src_path.joinpath(\"configs/demo/data-mnist.json\")\n",
    "data_configs = load_from_json(file)\n",
    "\n",
    "# Load model configs\n",
    "model_configs = load_from_json(src_path.joinpath(\"configs/demo/model-mnist.json\"))\n",
    "attack_configs = load_from_json(src_path.joinpath(\"configs/demo/attack-zk-mnist.json\"))\n",
    "\n",
    "output_path = project_path.joinpath(\"data\")\n",
    "\n",
    "# Fix configs\n",
    "# i.e. remove relative paths\n",
    "data_configs['dir'] = str(project_path.joinpath(\"data\"))\n",
    "data_configs['sub_dir'] = str(project_path.joinpath(\"results\"))\n",
    "\n",
    "model_configs['dir'] = str(project_path.joinpath(\"models/cnn\"))\n",
    "model_configs['pgd_trained'] = str(project_path.joinpath(\"models/baseline\", Path(model_configs['pgd_trained']).name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ab456fb02144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load benign samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bs_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load benign samples\n",
    "file = project_path.joinpath(data_configs['dir'], data_configs['bs_file'])\n",
    "X_bs = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-040e2d1014de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load true labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load true labels\n",
    "file =  project_path.joinpath(data_configs['dir'], data_configs['label_file'])\n",
    "labels = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-aef11596627a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'um_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_lenet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_file = project_path.joinpath(model_configs['dir'], model_configs['um_file'])\n",
    "target = load_lenet(file=model_file, wrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ad335c278f3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the benign samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# TODO: Is this redundant duplicate code?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bs_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the benign samples\n",
    "# TODO: Is this redundant duplicate code?\n",
    "data_file = project_path.joinpath(data_configs['dir'], data_configs['bs_file'])\n",
    "data_bs = np.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7cdbaff6e60b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load true labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlabel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dir'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_configs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load true labels\n",
    "label_file = project_path.joinpath(data_configs['dir'], data_configs['label_file'])\n",
    "labels = np.load(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dd3da5a37685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove files in data with prefix \"task1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"task1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove files in data with prefix \"task1\"\n",
    "\n",
    "for f in os.listdir(output_path):\n",
    "    if f.startswith(\"task1\"):\n",
    "        os.remove(output_path.joinpath(f))\n",
    "        \n",
    "        print(f\"Removed {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the adverserial examples\n",
    "\n",
    "This section of the code is where the adverserial examples are generated. The team has decided to use the Fast Gradient Sign Method(FGSM), Project Gradient Descent(PGD), and Basic Iteractive Method(BIM) attacks to create the adversary examples. Each of the methods were given four variants each with different values to see how much the original image is perturbed per point value and how it affects the error rate of the model.\n",
    "\n",
    "#Fast Gradient Sign Method\n",
    "\n",
    "\n",
    "\n",
    "#Project Gradient Descent\n",
    "\n",
    "#Basic Iteractive Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-65259512a0e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate adversarial examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m ae_files = generate_ae(model=target, data=data_bs,labels=labels, attack_configs=attack_configs,\n\u001b[0m\u001b[0;32m      3\u001b[0m             save=True, output_dir=output_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_ae' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate adversarial examples\n",
    "ae_files = generate_ae(model=target, data=data_bs,labels=labels, attack_configs=attack_configs,\n",
    "            save=True, output_dir=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to remove .ipynb_checkpoints\n",
      "Failed to remove evaluation\n",
      "Cleared results folder\n"
     ]
    }
   ],
   "source": [
    "# Clear results folder\n",
    "result_files = os.listdir(project_path.joinpath(\"results\"))\n",
    "\n",
    "for f in result_files:\n",
    "    file = project_path.joinpath(\"results\", f)\n",
    "    \n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except Exception:\n",
    "        print(f\"Failed to remove {f}\")\n",
    "\n",
    "print(\"Cleared results folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ae_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2016e725a14c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Replace adversarial examples with the newly generated examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ae_files'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mae_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ae_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace adversarial examples with the newly generated examples\n",
    "data_configs['ae_files'] = ae_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1076a8a8ab5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load configs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"configs/demo/athena-mnist.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrans_configs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Load configs\n",
    "file = src_path.joinpath(\"configs/demo/athena-mnist.json\")\n",
    "trans_configs = load_from_json(file)\n",
    "\n",
    "output_dir = project_path.joinpath(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Evaluate model\n",
    "evaluate(trans_configs=trans_configs,\n",
    "         model_configs=model_configs,\n",
    "         data_configs=data_configs,\n",
    "         save=False,\n",
    "         output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
