{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Team LANS - Task 2 Report</h1>\n",
    "<h4 align=\"center\">Team member: Landin Thorsted, Alex Tsai, Nick Bautista, Savannah Sun</h4>\n",
    "<h4 align=\"center\">CSCE 585 - Machine Learning System</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Much like Task 1, the goal is to generate adverserial examples and evaluate them on the Undefended Model, the Vanilla Athena, and the PGD-ADT. The difference with task 1 comes from the context that the attacks are given. In task 1, it is given zero knowledge about the model. For task 2, the attack is given knowledge about the model and its defenses, a white-box attack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Example \n",
    "Inputs to machine learning models that an attacker has deliberately programmed to cause the model to make a mistake are adversarial examples; they are like optical illusions for computers. Or in the other way, an adversarial example is a sample of input data that has been changed very slightly in such a way as to allow machine learning to misclassify it. For example, If we give an adversarial example to an image, the AE will overlay on a the original picture, which will cause a classifier to miscategorize a panda as a gibbon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Settings\n",
    "\n",
    "1. Use the same subsample from Task 1.\n",
    "2. Load a pool of weak defenses into an ensemble. In this case, 5 weak defenses are loaded.\n",
    "3. Generate the adverserial examples with the Vanilla Athena as the target.\n",
    "4. Save the results of the generation(includes sample images of the perturbation)\n",
    "5. Evaluate the generated AEs on the undefended model, the Vanilla Athena with the 5 WDs pooled in step two, and the PGD-ADT.\n",
    "6. Save the evaluation results in JSON format.\n",
    "\n",
    "## Elaboration\n",
    "\n",
    "For this task, to keep things orderly, we copied the same subsamples from Task 1 to Task 2. This also makes comparison between the two tasks much more accurate as they are using the same subsamples. \n",
    "\n",
    "* The subsamples can be found under /Task2_update/data/subsample/subsamples-mnist-ratio_0.05-590500.359/\n",
    "* The sublabels can be found under /Task2_update/data/subsample/sublabels-mnist-ratio_0.05-590500.359/\n",
    "\n",
    "After getting the subsamples, we loaded a pool of 5 weak defenses(WD) to the Vanilla Athena. These are all arbitrarily selected. The team made sure that the WDs are diverse. Initially, the team wanted to load a pool of 15 weak defenses into the ensemble but due to the computational costs that loading additional weak defenses add, we decided to stick with the first 5 weak defenses to be loaded. This ensemble is then set as the target for generating the adverserial examples. By doing this, the attack gets all the information about the model it is attacking. \n",
    "\n",
    "The weak defenses selected are:\n",
    "\n",
    "* _model-mnist-cnn-rotate90.h5_\n",
    "* _model-mnist-cnn-shift_left.h5_\n",
    "* _model-mnist-cnn-flip_horizontal.h5_\n",
    "* _model-mnist-cnn-affine_vertical_compress.h5_\n",
    "* _model-mnist-cnn-morph_erosion.h5_\n",
    "\n",
    "After setting the target, the team generated adverserial examples using the Fast Gradient Sign Method, and the Project Gradient Descent attack. Each attack will have 5 different epsilon values as the parameter to generate the adverserial example. For each epsilon value, there will be 2 variants. One will have be extended with the Expectation over Transformation(EOT) algorithm, while the other will not. This will help determine how EOT affects the attack's potency. With these, the total amount of variants will be 20. \n",
    "\n",
    "The configurations of the attacks can be found in:\n",
    "\n",
    "* _/Task2_update/configs/attack-wb-mnist.json_\n",
    "* _/Task2_update/configs/attack-wb-mnist.json_\n",
    "\n",
    "The generated adverserial examples can be found under _/Task2_update/data/adversary_examples/_\n",
    "\n",
    "Afterwards, the generated AEs are evaluated on the undefended model, the Vanilla Athena with the same 5 WDs that the team generated AEs on, and the PGD-ADT.\n",
    "\n",
    "These can be found under _/models/cnn/_\n",
    "\n",
    "The results of the evaluation are found under _/Task2_update/results/ae_evaluation_results.json/_\n",
    "\n",
    "If one is to read the Task 1 Report, the weak defenses selected in this Task are the same as Task 1. This is decided so that the team can compare a zero-knowledge attack versus a white-box attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation over Transformation Algorithm\n",
    "\n",
    "Expectation Over Transformation (EOT) is a general framework for allowing the construction of adversarial examples that remain adversarial over a chosen transformation distribution. EOT models perturbations within the optimization procedure, by using the previously mentioned distribution, T, which is of transformation t taking input t(x'). It also aims to constrain the expected distance adversarial and original inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Method\n",
    "\n",
    "The FGSM is a computationally efficient method for generating adversarial examples, which takes the sign of the gradient and multiplies it by an epsilon and adds the result to the image. On small values of epsilon, the image is visually similar to the human eye but its classification can be completely changed, or the confidence level of the prediction is reduced. The \"fast\" in its name comes from the fact that it does not do an iterate procedure in order to generate adverserial examples which makes it faster than many other methods. This can be summarised using the following expression:\n",
    "\n",
    "<h3 align=\"center\"><b>\n",
    "\\begin{equation*}\n",
    "adv_x = x + \\epsilon*sign(\\nabla_x L(\\theta, x, y))\n",
    "\\end{equation*}</b></h3> \n",
    "\n",
    "where x is an example, y is the lable of x,θ is the model parameters, J(θ,x,y) is the loss function used to generate adversarial examlpe, adv_x is the adversary image and ϵ is a constant which also is a multiplier to contral the size of perturbations. The only drawback is that the success rate is often lower than other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>FGSM Adverserial Example Images and Error Rates</center></h1>\n",
    "\n",
    "| Parameter | FGSM EOT OFF Image |FGSM EOT ON Image |\n",
    "| :---: | :---: | :---: |\n",
    "|<b>eps0.1</b> | <img src=\"images/FGSM_eps0.1-0-EOT_OFF.png\">  <img src=\"images/FGSM_eps0.1-1-EOT_OFF.png\"> |<img src=\"images/FGSM_eps0.1-0-EOT_ON.png\">  <img src=\"images/FGSM_eps0.1-1-EOT_ON.png\">  |\n",
    "|<b>eps0.15</b> | <img src=\"images/FGSM_eps0.15-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.15-1-EOT_OFF.png\"> | <img src=\"images/FGSM_eps0.15-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.15-1-EOT_ON.png\"> |\n",
    "|<b>eps0.2</b>| <img src=\"images/FGSM_eps0.2-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.2-1-EOT_OFF.png\"> | <img src=\"images/FGSM_eps0.2-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.2-1-EOT_ON.png\"> |\n",
    "|<b>eps0.23</b>| <img src=\"images/FGSM_eps0.23-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.23-1-EOT_OFF.png\"> | <img src=\"images/FGSM_eps0.23-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.23-1-EOT_ON.png\"> |\n",
    "|<b>eps0.25</b> | <img src=\"images/FGSM_eps0.25-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.25-1-EOT_OFF.png\"> | <img src=\"images/FGSM_eps0.25-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.25-1-EOT_ON.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM Evaluation Table\n",
    "\n",
    " <h3  align=\"center\">EOT:OFF</h3>\n",
    " \n",
    "| Epsilon Value | UM | Vanilla Athena | PGD-ADT |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| eps0.1| 0.020161290322580645 | 0.006048387096774193 | 0.006048387096774193 |\n",
    "| eps0.15| 0.03024193548387097 | 0.016129032258064516 | 0.010080645161290322 |\n",
    "| eps0.2| 0.07862903225806452 | 0.056451612903225805 | 0.024193548387096774 |\n",
    "| eps0.23| 0.1431451612903226 | 0.11290322580645161 | 0.036290322580645164 |\n",
    "| eps0.25| 0.19153225806451613 | 0.16129032258064516 | 0.046370967741935484 |\n",
    "\n",
    "<h3 align=\"center\">EOT:ON</h3>\n",
    "\n",
    "| Epsilon Value | UM | Vanilla Athena | PGD-ADT |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| eps0.1| 0.018145161290322582 | 0.010080645161290322 | 0.004032258064516129 |\n",
    "| eps0.15| 0.04032258064516129  | 0.02217741935483871 | 0.006048387096774193 |\n",
    "| eps0.2| 0.06451612903225806 | 0.07661290322580645 |0.016129032258064516 |\n",
    "| eps0.23| 0.12298387096774194 | 0.1592741935483871 | 0.020161290322580645 |\n",
    "| eps0.25| 0.19959677419354838 | 0.21975806451612903 | 0.03024193548387097 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOT ON vs OFF Analysis\n",
    "\n",
    "In Task 1, EOT has negatively affected the evaluation of the generated adverserial examples very heavily. By giving the model's information to the attacker, as per expectations, EOT makes the adverserial example more robust. In this case, in all evaluations in the Vanilla Athena which we gave information to the attacker to, the attack's potency is greatly enhanced in comparison to the adversary examples without EOT. However, it seems like other than the evaluation in the Vanilla Athena, the attack's potency is still weakened. The only exception to this is when the epsilon is at 0.25, the highest we tested, where in the evaluation for the undefended model, having EOT on is actually higher than having EOT off. If that trend follows throughout greater epsilon values, that means having EOT on for attacks with high epsilon values is stronger than without. Against defended models where the attack does not know the exact information, EOT weakens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 align=\"center\">FGSM Evaluation Graph(Whitebox)</h1>\n",
    "\n",
    "| ZK Threat Model Graph | White-box Attack |\n",
    "| :---: | :---: |\n",
    "| <img src =\"../Task1_update/graphs/FGSM.png\"> | <img src =\"graphs/FGSM.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Knowledge Threat Model vs Optimization Based White-box Attack\n",
    "\n",
    "One thing that the team has noticed with this graph is that everything is a lot lower in the white-box attack than it is on the zero-knowledge threat model. This is to be expected as leaving a model undefended makes it very vulnerable to \n",
    "adverserial attacks. The biggest drop in the error rate comes from the evaluation of the undefended model with EOT off. In the zero-knowledge threat model, the error rate goes all the way up to 0.8. In the white-box attack, the highest the error rate got is 0.22 which is when the EOT is on and the epsilon is set to 0.25. In the topic of EOT being on,  based on this graph, it seems as though the error rate between the zero-knowledge threat model and the white-box attack is almost unchanged for evaluation with defenses put in. In the case of the model being evaluated is the same as the target, the error rate is actually higher in the white-box attack than it is when the target is undefended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gradient Descent\n",
    "\n",
    "The PGD attack is an iterative attack, which can be seen as a replica of FGSM -- K-FGSM(K represents the numbers of iterations). PGD attack generates adversarial examples by iteratively applying FGSM and projecting the perturbed example to be a valid example multiple times. The general idea of FGSM is that one iteration is a big step while PGD does multiple iterations. Each iteration is a small step, and each iteration will disturb clip to the specified range.This can be summarised using the following expression:\n",
    "\n",
    "<h3 align=\"center\">\n",
    "\\begin{equation*}\n",
    "x^{t+1}   =  \\prod_{x+s}(x^t+\\alpha sgn(\\nabla_x L(\\theta, x, y)))\n",
    "\\end{equation*}</h3>\n",
    "\n",
    "If the target model is a linear model, then FGSM will work fine with that, because at this time the derivative of loss to the input is fixed. In other words, the direction of loss is clear, even if you iterate multiple times, the direction of the disturbance also will not change. For a non-linear model, the direction may not be completely correct after only one iteration, and that's the time PGD will replace FGSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 align=\"center\">PGD Adverserial Example Images and Error Rates</h1>\n",
    "\n",
    "| Parameter | PGD EOT OFF Image | PGD EOT ON Image |\n",
    "| :---: | :---: | :---: | \n",
    "|<b>eps0.08</b> | <img src=\"images/PGD_eps0.08-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.08-1-EOT_OFF.png\"> |<img src=\"images/PGD_eps0.08-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.08-1-EOT_ON.png\"> |\n",
    "|<b>eps0.12</b> | <img src=\"images/PGD_eps0.12-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.12-1-EOT_OFF.png\"> | <img src=\"images/PGD_eps0.12-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.12-1-EOT_ON.png\"> |\n",
    "|<b>eps0.15</b> | <img src=\"images/PGD_eps0.15-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.15-1-EOT_OFF.png\"> |<img src=\"images/PGD_eps0.15-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.15-1-EOT_ON.png\"> |\n",
    "|<b>eps0.17</b> | <img src=\"images/PGD_eps0.17-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.17-1-EOT_OFF.png\"> |<img src=\"images/PGD_eps0.17-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.17-1-EOT_ON.png\"> |\n",
    "|<b>eps0.2</b> | <img src=\"images/PGD_eps0.2-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.2-1-EOT_OFF.png\"> | <img src=\"images/PGD_eps0.2-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.2-1-EOT_ON.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">PGD EOT:OFF</h3>\n",
    "\n",
    "| Epsilon Value | UM | Vanilla Athena | PGD-ADT |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| eps0.08| 0.008064516129032258 | 0.006048387096774193 | 0.004032258064516129 |\n",
    "| eps0.12| 0.024193548387096774 | 0.012096774193548387 | 0.006048387096774193 |\n",
    "| eps0.15| 0.034274193548387094 | 0.02217741935483871 | 0.008064516129032258 |\n",
    "| eps0.17| 0.038306451612903226 | 0.03225806451612903 | 0.014112903225806451 |\n",
    "| eps0.20| 0.06653225806451613 | 0.04032258064516129 | 0.016129032258064516 |\n",
    "\n",
    "<h3 align=\"center\">PGD EOT:ON</h3>\n",
    "\n",
    "| Epsilon Value | UM | Vanilla Athena | PGD-ADT |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| eps0.08| 0.008064516129032258 | 0.008064516129032258 | 0.004032258064516129 |\n",
    "| eps0.12| 0.014112903225806451 | 0.010080645161290322 | 0.006048387096774193 |\n",
    "| eps0.15| 0.024193548387096774 | 0.020161290322580645 | 0.006048387096774193 |\n",
    "| eps0.17| 0.02620967741935484 | 0.03225806451612903 | 0.010080645161290322 |\n",
    "| eps0.20| 0.038306451612903226 | 0.06653225806451613 | 0.012096774193548387 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOT ON vs OFF Analysis\n",
    "\n",
    "PGD follows the same trend as FGSM when it comes to the difference between EOT being on and EOT being off. However, there is an interesting thing that came up with PGD. In the epsilon value 0.08, the evaluation for the Vanilla Athena is higher when EOT is on than when it is off. However, for epsilon values 0.12 and 0.15, EOT off is actually greater by roughly 0.002 then they equalize back up at 0.17. In the final epsilon step that we tested(0.20), EOT on is back to being higher than EOT off. Again, if this trend continues, this suggests that EOT becomes more effective as the epsilon values get higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">PGD Evaluation Graph(White-box)</h3>\n",
    "\n",
    "| ZK Threat Model Graph | White-box Attack |\n",
    "| :---: | :---: |\n",
    "| <img src =\"../Task1_update/graphs/PGD.png\"> | <img src =\"graphs/PGD.png\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGD Analysis\n",
    "\n",
    "Just like in the graph for FGSM, PGD also suffers a huge drop in the error rate in the white-box attack evaluated on the undefended model. For the purposes of analysis, we are going to ignore the undefended model's evaluation as the zero-knowledge threat model evaluation on the UM is a very big outlier. In the case of EOT being off, the error rate for most of the evaluations have gone down. The only epsilon value that did not follow that trend is at 0.08 where the error rate is actually higher when evaluated on the Vanilla Athena. In the case of EOT being on, for the evaluation on the undefended model, it has gone down by a significant amount. In the largest epsilon value, 0.2, the error rate has went down from almost 11% to only 3.8%. Below is the evaluation for PGD-ADT for the zero-knowledge threat model versus the white-box attack.\n",
    "\n",
    "| Epsilon Value | ZK | White-box |\n",
    "| :---: | :---: | :---: |\n",
    "| 0.08 | 0.006072874493927126 | 0.004032258064516129 |\n",
    "| 0.12 | 0.006072874493927126 | 0.006048387096774193 | \n",
    "| 0.15 | 0.006072874493927126 | 0.006048387096774193 |\n",
    "| 0.17 | 0.006072874493927126 | 0.010080645161290322 |\n",
    "| 0.20 | 0.010121457489878543 | 0.012096774193548387 |\n",
    "\n",
    "What is interesting is that, in ZK, even when the epsilon value was raised by 0.09, it remained constant. As the epsilon value increased, the white-box attack started overtaking ZK's error rate by a substantial amount(2%). \n",
    "\n",
    "When evaluated on the targeted model, EOT can really see how EOT can effectively make an attack stronger. Compared to EOT off, the error rate is a lot higher despite having the same epsilon values. As the epsilon value got increased, the gap between the error rate of EOT being on and EOT being off got higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution: \n",
    "\n",
    "* Landin Thorsted: Helped in doing research on the attacks and EOT. Helped in the introductions for the attacks and EOT. Helped in analyzing data. \n",
    "* Alex Tsai: Generated the adverserial examples, wrote the script to generate the graphs for the report.\n",
    "* Nick Bautista: Handled the scripts, wrote the experimental settings in the report, analyzed the data.\n",
    "* Yiqian Sun: Made the tables, helped in analyzing data, wrote the introductions for the attacks as well as the EOT algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Anish Athalye, Logan Engstrom,Andrew Ilyas, Kevin Kwok.\"Synthesizing Robust Adversarial Examples\". June, 7, 2018.\n",
    "https://arxiv.org/pdf/1707.07397.pdf.\n",
    "\n",
    "2.Synthesizing robust adversarial examples(2018) prabhant.[Source Code]\n",
    "https://github.com/prabhant/synthesizing-robust-adversarial-examples.\n",
    "\n",
    "3.Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy. \"Explaining and Harnessing Adversarial Examples\" _Conell University_, Mar 20, 2015. \n",
    "https://arxiv.org/abs/1412.6572\n",
    "\n",
    "4.Anish Athalye , Logan Engstrom, Andrew Ilyas , Kevin Kwok. \"Synthesizing Robust Adversarial Examples\". Jun 7, 2018.\n",
    "https://arxiv.org/pdf/1707.07397.pdf\n",
    "\n",
    "5.Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi. \"ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense\" _Conell University_, Oct 16, 2020.\n",
    "https://arxiv.org/abs/2001.00308\n",
    "\n",
    "6.Shuangtao Li, Yuanke Chen, Yanlin Peng, Lin Bai. \"Learning More Robust Features with Adversarial Training\" \n",
    "https://www.arxiv-vanity.com/papers/1804.07757/\n",
    "\n",
    "7.“Adversarial Example Using FGSM\" *TensorFlow*.\n",
    "www.tensorflow.org/tutorials/generative/adversarial_fgsm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
