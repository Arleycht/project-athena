{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Team LANS - Task 1 Report</center></h1>\n",
    "<h4><center>Team member: Landin Thorsted, Alex Tsai, Nick Bautista, Savannah Sun</center></h4>\n",
    "<h4><center>CSCE 585 - Machine Learning System</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "The objective of the task is to craft adverserial examples based on the zero-knowledge threat model and evaluate them on the undefended model, the Vanilla Athena, and the PGD-ADT. \n",
    "\n",
    "The script for the whole process is found in _/Task1_update/ExperimentScript.ipynb_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Example \n",
    "Inputs to machine learning models that an attacker has deliberately programmed to cause the model to make a mistake are adversarial examples; they are like optical illusions for computers. Or in the other way, an adversarial example is a sample of input data that has been changed very slightly in such a way as to allow machine learning to misclassify it. For example, If we give an adversarial example to an image, the AE will overlay on a the original picture, which will cause a classifier to miscategorize a panda as a gibbon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Settings\n",
    "\n",
    "In order to produce the results, these steps were followed.\n",
    "\n",
    "1. Take a subsample from the overall sample at a ratio of 0.05\n",
    "2. Set the undefended model as the target.\n",
    "3. Using the target, generate the adversary examples(AEs).\n",
    "4. Save the results of the generation(includes sample images of the perturbation)\n",
    "5. Load a pool of 5 weak defenses(WDs) that are arbitrarily selected for the Ensemble.\n",
    "6. Evaluate the generated AEs on the undefended model, the Vanilla Athena with the 5 WDs pooled in step five, and the PGD-ADT.\n",
    "7. Save the evaluation results in JSON format.\n",
    "\n",
    "\n",
    "## Elaboration \n",
    "\n",
    "Due to time constraint, generating adverserial examples for the whole sample would be impossible due to the large amount of computational cost it would have to perform the entire process. Thus, the team has decided to take subsamples from the total sample at a ratio of 0.05. This greatly reduces the amount of time it would take to generate, and evaluate the adverserial examples(AEs). The subsampling script can be found in the Task1_update folder. \n",
    "\n",
    "* The subsamples can be found under _/Task1_update/data/subsample/subsamples-mnist-ratio_0.05-590500.359/_\n",
    "* The sublabels can be found under _/Task1_update/data/subsample/sublabels-mnist-ratio_0.05-590500.359/_\n",
    "\n",
    "Once the subsamples were taken, the undefended model was loaded as the target. Afterwards, AEs were generated using the Fast Gradient Sign Method(FGSM), and the Project Gradient Descent(PGD) attacks. Each attack had 10 different variants. These variants are generated using different parameters for each attack.  Half of the variants for each attack are extended with the Expectation over Transformation algorithm. The other half does not. This is done so that the results could be compared to see how the extension of EOT into the attacks affect the error rate on the evaluation of the AEs. This makes a total of 10 for each type of attack and a total of 20 overall for both.\n",
    "\n",
    "From the project folder(project-athena), the configurations of the attacks can be found under the following paths:\n",
    "\n",
    "* Attacks with EOT off: /Task1_update/attack-zk-mnist.json\n",
    "* Attacks with EOT on: /Task1_update/attack-zk-mnist-EOT_ON.json\n",
    "\n",
    "The AEs generated using the FGSM and PGD attacks with the undefended model as the target can be found under\n",
    "_/Task1_update/data/adversary_examples/_\n",
    "\n",
    "Sample images of how the attacks affected the original image are also saved. This is done so that we can see how the attack messes with the image in order to fool the model. These images are saved under _/Task1_update/images_\n",
    "\n",
    "After generating the AEs, they are then evaluated into the undefended model, the Vanilla Athena consisting of 5 weak defenses, and the PGD-ADT. This is done so that we can compare how accurate the model is given the AEs generated previously. The weak defenses chosen for the ensemble are arbitrarily selected. At first, the team wanted to load 15 weak defenses but that adds computational cost to the process which the team did not have time for. Thus, the team decided to keep only the first 5 of the weak defenses. \n",
    "\n",
    "The weak defenses selected are:\n",
    "\n",
    "* _model-mnist-cnn-rotate90.h5_\n",
    "* _model-mnist-cnn-shift_left.h5_\n",
    "* _model-mnist-cnn-flip_horizontal.h5_\n",
    "* _model-mnist-cnn-affine_vertical_compress.h5_\n",
    "* _model-mnist-cnn-morph_erosion.h5_\n",
    "\n",
    "These can be found under _/models/cnn/_\n",
    "\n",
    "The results of the evaluation are found under _/Task1_update/results/ae_evaluation_results.json/_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOT Attack\n",
    "\n",
    "Expectation Over Transformation (EOT) is a method of generating robust adversarial examples that are resistant to common 2D and 3D transformations, thus being able to remain adversarial in real conditions. In this task... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Method\n",
    "\n",
    "The FGSM is a computationally efficient method for generating adversarial examples, which takes the sign of the gradient and multiplies it by an epsilon and adds the result to the image. On small values of epsilon, the image is visually similar to the human eye but its classification can be completely changed, or the confidence level of the prediction is reduced. The \"fast\" in its name comes from the fact that it does not do an iterate procedure in order to generate adverserial examples which makes it faster than many other methods. This can be summarised using the following expression:\n",
    "\n",
    "<h3 align=\"center\"><b>adv_x = x + ϵ*sign(∇xJ(θ,x,y))</b></h3> \n",
    "\n",
    "where x is an example, y is the lable of x,θ is the model parameters, J(θ,x,y) is the loss function used to generate adversarial examlpe, adv_x is the adversary image and ϵ is a constant which also is a multiplier to contral the size of perturbations. The only drawback is that the success rate is often lower than other methods.\n",
    "\n",
    "## Results:\n",
    "<h3><center>FGSM Adverserial Example Images and Error Rates</center></h3>\n",
    "\n",
    "| Parameter | FGSM EOT OFF Image | FGSM EOT OFF Error Rate |FGSM EOT ON Image | FGSM EOT ON Error Rate |\n",
    "| :---: | :---: | :---: | :---: |:---: |\n",
    "|<b>eps0.1</b> | <img src=\"images/FGSM_eps0.1-0-EOT_OFF.png\">  <img src=\"images/FGSM_eps0.1-1-EOT_OFF.png\"> | <b>0.262</b> |<img src=\"images/FGSM_eps0.1-0-EOT_ON.png\">  <img src=\"images/FGSM_eps0.1-1-EOT_ON.png\">  | <b>0.054</b> | \n",
    "|<b>eps0.15</b> | <img src=\"images/FGSM_eps0.15-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.15-1-EOT_OFF.png\"> | <b>0.56</b> | <img src=\"images/FGSM_eps0.15-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.15-1-EOT_ON.png\"> | <b>0.112</b> |\n",
    "|<b>eps0.2</b>| <img src=\"images/FGSM_eps0.2-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.2-1-EOT_OFF.png\"> | <b>0.746</b> | <img src=\"images/FGSM_eps0.2-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.2-1-EOT_ON.png\"> | <b>0.21</b> |\n",
    "|<b>eps0.23</b>| <img src=\"images/FGSM_eps0.23-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.23-1-EOT_OFF.png\"> | <b>0.822</b> | <img src=\"images/FGSM_eps0.23-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.23-1-EOT_ON.png\"> | <b>0.326</b> |\n",
    "|<b>eps0.25</b> | <img src=\"images/FGSM_eps0.25-0-EOT_OFF.png\"> <img src=\"images/FGSM_eps0.25-1-EOT_OFF.png\"> | <b>0.85</b> | <img src=\"images/FGSM_eps0.25-0-EOT_ON.png\"> <img src=\"images/FGSM_eps0.25-1-EOT_ON.png\"> | <b>0.442</b> |\n",
    "\n",
    "Looking at the results, with increasing value of the epsilon, the error rate goes up and the image gets more distorted as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>FGSM Error Rate Graph</center></h3>\n",
    "\n",
    "<img src =\"graphs/FGSM-experimental.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FGSM, based on the graph above, we can see the adversarial examples in the context of a zero-knowledge threat model evaluated on 3 different models with EOT are more effective than without EOT. As the epsilon increasing, the difference between FGSM EOT ON and FGSM EOT OFF gets more obvious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>FGSM Evaluation Graph</center></h3>\n",
    "\n",
    "<img src =\"graphs/FGSM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion for FGSM Results\n",
    "\n",
    "As expected, the undefended model performs poorly against any type of adverserial example. The error rate greatly increases as each point of epsilon is added to the attack. This is the case when the attack is not extended with the EOT algorithm.  What is interesting, however, is that turning on the EOT greatly reduces the attack's potency. As per the graph, the smaller the epsilon, the greater the difference is between having EOT on and EOT off. As the epsilon gets larger, EOT on seems to be slowly catching up to the potency when EOT is off.\n",
    "\n",
    "In all epsilons tested, the Vanilla Athena performs worse than the PGD-ADT. This can be attributed to the fact that the number of weak defenses is insufficient for the attack. The number of weak defenses loaded into Athena for this task is only 5 which is only a small number of the total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gradient Descent\n",
    "\n",
    "The PGD attack is an iterative attack, which can be seen as a replica of FGSM -- K-FGSM(K represents the numbers of iterations). PGD attack generates adversarial examples by iteratively applying FGSM and projecting the perturbed example to be a valid example multiple times. The general idea of FGSM is that one iteration is a big step while PGD does multiple iterations. Each iteration is a small step, and each iteration will disturb clip to the specified range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3><center>PGD Adverserial Example Images and Error Rates</center></h3>\n",
    "\n",
    "| Parameter | PGD EOT OFF Image | PGD EOT OFF Error Rate | PGD EOT ON Image2 | PGD EOT ON Error Rate |\n",
    "| :---: | :---: | :---: | :---: | :---:|\n",
    "|<b>eps0.08</b> | <img src=\"images/PGD_eps0.08-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.08-1-EOT_OFF.png\"> | <b>0.428</b> |<img src=\"images/PGD_eps0.08-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.08-1-EOT_ON.png\"> | <b>0.02</b> |\n",
    "|<b>eps0.12</b> | <img src=\"images/PGD_eps0.12-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.12-1-EOT_OFF.png\"> | <b>0.844</b> | <img src=\"images/PGD_eps0.12-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.12-1-EOT_ON.png\"> | <b>0.036</b> |\n",
    "|<b>eps0.15</b> | <img src=\"images/PGD_eps0.15-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.15-1-EOT_OFF.png\"> | <b>0.954</b> |<img src=\"images/PGD_eps0.15-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.15-1-EOT_ON.png\"> | <b>0.046</b> |\n",
    "|<b>eps0.17</b> | <img src=\"images/PGD_eps0.17-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.17-1-EOT_OFF.png\"> | <b>0.99</b> |<img src=\"images/PGD_eps0.17-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.17-1-EOT_ON.png\"> | <b> 0.07</b> |\n",
    "|<b>eps0.2</b> | <img src=\"images/PGD_eps0.2-0-EOT_OFF.png\"> <img src=\"images/PGD_eps0.2-1-EOT_OFF.png\"> | <b>1.0</b> | <img src=\"images/PGD_eps0.2-0-EOT_ON.png\"> <img src=\"images/PGD_eps0.2-1-EOT_ON.png\"> | <b>0.122</b> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>PGD Error Rate Graph</center></h3>\n",
    "\n",
    "<img src =\"graphs/PGD-experimental.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, each point increase in epsilon increases the error rate by a substantial amount. Also if we compare same epsilon of PGD and FGSM, PGD has a lower error rate than the FGSM as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>PGD Evaluation Graph</center></h3>\n",
    "\n",
    "<img src =\"graphs/PGD.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion for PGD Results\n",
    "\n",
    "In comparison to FGSM, each point of epsilon value holds higher potency at making successful adverserial examples. This is expected because PGD is an iterative version of FGSM. As such, as can be seen, even with lower epsilon values, the error rate against the undefended model compared to FGSM is much higher. However, PGD seems to struggle more when it is evaluated against defenses. For FGSM, the error rate gets affected less when evaluated against models that are undefended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "<b> Landin Thorsted:\n",
    "    \n",
    "<b> Alex Tsai:\n",
    "    \n",
    "<b> Nick Bautista:\n",
    "    \n",
    "<b> Savannah Sun:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation\n",
    "\n",
    "StatQuest with Josh Starmer. \"Gradient Descent, Step-by-Step\"YouTube video, Feb 5, 2019.\n",
    "https://www.youtube.com/watch?v=sDv4f4s2SB8&ab_channel=StatQuestwithJoshStarmer\n",
    " \n",
    "Anish Athalye, Logan Engstrom,Andrew Ilyas, Kevin Kwok.\"Synthesizing Robust Adversarial Examples\". June, 7, 2018.\n",
    "https://arxiv.org/pdf/1707.07397.pdf.\n",
    "\n",
    "Synthesizing robust adversarial examples(2018) prabhant.[Source Code]\n",
    "https://github.com/prabhant/synthesizing-robust-adversarial-examples.\n",
    "\n",
    "Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy. \"Explaining and Harnessing Adversarial Examples\" _Conell University_, Mar 20, 2015. \n",
    "https://arxiv.org/abs/1412.6572\n",
    "\n",
    "Anish Athalye , Logan Engstrom, Andrew Ilyas , Kevin Kwok. \"Synthesizing Robust Adversarial Examples\". Jun 7, 2018.\n",
    "https://arxiv.org/pdf/1707.07397.pdf\n",
    "\n",
    "Ying Meng, Jianhai Su, Jason O'Kane, Pooyan Jamshidi. \"ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense\" _Conell University_, Oct 16, 2020.\n",
    "https://arxiv.org/abs/2001.00308\n",
    "\n",
    "Shuangtao Li, Yuanke Chen, Yanlin Peng, Lin Bai. \"Learning More Robust Features with Adversarial Training\" \n",
    "https://www.arxiv-vanity.com/papers/1804.07757/\n",
    "\n",
    "“Adversarial Example Using FGSM\" TensorFlow, www.tensorflow.org/tutorials/generative/adversarial_fgsm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
